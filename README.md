# Nano-vLLM (macOS/MPS Fork)

> **Note:** This is a community fork of [nano-vllm](https://github.com/GeeeekExplorer/nano-vllm) specifically optimized for **macOS** with Metal Performance Shaders (MPS). All code related to CUDA, Linux, and distributed training has been removed to keep it lightweight. This fork is intended for Apple Silicon and Intel Macs using PyTorch MPS.

A lightweight vLLM implementation, now macOS/MPS only.

## Key Features

* ðŸš€ **Fast offline inference on macOS** â€“ Optimized for MPS, competitive speeds on Apple hardware
* ðŸ“– **Readable codebase** â€“ Clean implementation in ~1,200 lines of Python
* âš¡ **MPS-optimized** â€“ Efficient KV cache, correct causal attention, eager execution

## Installation

```bash
pip install git+https://github.com/jacko06v/nano-vllm-macos.git
```

## Manual Download

If you prefer to download model weights manually:
```bash
huggingface-cli download --resume-download Qwen/Qwen3-0.6B \
  --local-dir ~/huggingface/Qwen3-0.6B/ \
  --local-dir-use-symlinks False
```

## Quick Start

See `example_macos.py` for usage on macOS. The API is similar to vLLM, but now optimized for MPS:


**Test Configuration:**
- Hardware: MacBook Pro M1/M2/M3 (16GB)
- Model: Qwen3-0.6B
- Input: Example prompt, output ~13-15 tokens/s on MPS

**Note:** Performance on NVIDIA/CUDA GPUs or Linux is not supported in this fork.

## Star History

[![Star History Chart](https://api.star-history.com/svg?repos=jacko06v/nano-vllm-macos&type=Date)](https://www.star-history.com/#jacko06v/nano-vllm-macos&Date)



Hi! I really appreciate your fork. I'm currently working on running nano-vllm without CUDA on a CPU-only x86 Linux system.  
Does this fork support `torch.device("cpu")`? Or should any additional changes be made to make it work?Because i am using intel cpu on thinkpad so i am not sure it can run like Mac intels you said 

Thanks in advance!

